
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
1. INTRODUCCIÃ“N
En el tema anterior se describiÃ³ de forma global el concepto de Aprendizaje AutomÃ¡tico
focalizando en los mÃ©todos de Aprendizaje Supervisado los cuales utilizan informaciÃ³n
etiquetada para aprender diferentes modelos basados en clasificaciÃ³n y regresiÃ³n.
Lamentablemente el nÃºmero de conjuntos de datos correctamente etiquetados es bastante
pequeÃ±o lo cual produce que la potencia que ofrecen los diferentes algoritmos de Aprendizaje
Supervisado desaparezca debido a la complejidad que supone conseguir buenos conjuntos de
datos que estÃ©n correctamente etiquetados. AdemÃ¡s el proceso de etiquetado suele presentar
algunos problemas que aumentan su complejidad o disminuyen su efectividad. Esto es debido a
que este proceso (manual o automÃ¡tico) puede haberse realizado de manera incorrecta haciendo
que los datos sean totalmente inservibles o los datos puede que estÃ©n sesgado o que existen
situaciones en la cuales no es posible realizar un etiquetado de datos debido a la naturaleza a la
naturaleza, cantidad o complejidad de los datos. Este y otros problemas derivados del
funcionamiento de los algoritmos los investigadores en Inteligencia Artificial decidieron intentar
crear otros tipos de mÃ©todos que no necesitaran de informaciÃ³n correctamente etiquetada.
Los intentos por resolver estos problemas dieron lugar a la apariciÃ³n de mÃ©todos de Aprendizaje
no supervisado los cuales eran capaces de identificar caracterÃ­sticas comunes de las instancias
de entrenamiento que les permiten agrupar la informaciÃ³n en conjuntos o clases de manera
automÃ¡tica. AdemÃ¡s los diferentes estudios en otras Ã¡reas del proceso de aprendizaje de los
seres vivos dieron lugar a la apariciÃ³n de los algoritmos de Aprendizaje por Refuerzo los cuales
intentaban imitar el proceso de aprendizaje de los seres vivos los cuales son capaces de
aprender conceptos mediante la utilizaciÃ³n de recompensas que permitÃ­an reforzar aquella
informaciÃ³n que debÃ­a ser aprendida. A pesar de los avances de estos dos nuevos tipos de
familias de mÃ©todos seguÃ­an existiendo limites al proceso de aprendizaje automÃ¡tico debido
principalmente a la complejidad que suponÃ­a extraer caracterÃ­sticas Ãºtiles, tanto de forma manual
como de forma automÃ¡tica, para identificar ciertos elementos que permitan aprender conceptos
mÃ¡s complejos, lo que supuso una disminuciÃ³n en la producciÃ³n de nuevas tÃ©cnicas de
aprendizaje automÃ¡tica. Esta congelaciÃ³n temporal en la apariciÃ³n de nuevas tÃ©cnicas de
aprendizaje finalizÃ³ en 2006 cuando el aprendizaje profundo demostrÃ³ que ciertas arquitecturas
de redes de neuronas eran capaces de extraer caracterÃ­sticas de manera automÃ¡tica y aprender
modelos complejos.

4Inteligencia artificial â€“ Apredizaje automÃ¡tico II
En la Figura se presentan diferentes tipos de algoritmos de Aprendizaje AutomÃ¡tico distribuidos
por familia (Supervisado, No supervisado y Por refuerzo) y por tÃ©cnica (RegresiÃ³n, ClasificaciÃ³n
y Agrupamiento). En esta figura sÃ³lo se presentan algunos de las tÃ©cnicas mÃ¡s utilizados, pero
existen muchas mÃ¡s e incluso diferentes versiones de cada una de ellas. En rojo se marcan los
diferentes tÃ©cnicas que serÃ¡n descritas de forma detallada en este tema.
2. APRENDIZAJE NO SUPERVISADO
El aprendizaje no supervisado [13] es una de las familias de mÃ©todos de aprendizaje donde el
proceso de aprendizaje se realiza mediante la utilizaciÃ³n de instancias (ejemplos) no etiquetadas.
Este tipo de modelo de aprendizaje consiste en definir la informaciÃ³n del entorno mediante sÃ³lo
informaciÃ³n de entrada de manera que el resultado de salida (clase o valor) de todas las
instancias del conjunto de entrenamiento es definida en base a las diferentes similitudes que
pueden ser identificadas entre los diferentes conjuntos de entrenamiento. Esto implica que los
valores y/o el nÃºmero de las posibles salidas de los modelos generados por los mÃ©todos de este
5
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
de mÃ©todos es totalmente desconocido. Dependiendo del mÃ©todo a utilizar puede que el nÃºmero
de posibles salidas sea definido a priori por el usuario. Los diferentes elementos utilizados
(categorÃ­as, instancias, conjunto de entrenamiento, algoritmo, etc.) en el proceso de aprendizaje
de este tipo de mÃ©todos es exactamente igual que en el aprendizaje supervisado con la salvedad
de que no suele existir una fase de test debido a que no se conocen los posibles valores de
salida que van a obtenerse tras el proceso de aprendizaje.
2.1 Algoritmos K-medias
El algoritmo K-medias o algoritmo de Lloyd [1] es un mÃ©todo de aprendizaje no supervisado
basado en el concepto de agrupamiento o â€œclusteringâ€ de instancias similares en base a su
distribuciÃ³n espacial en el espacio de las variables (caracterÃ­sticas) que definen la estructura de
las diferentes instancias. El proceso de agrupamiento consiste en construir o identificar un
nÃºmero finito de conjuntos, grupos o â€œclustersâ€ (Celdas de Voronoi) en los cuales se encuentran
distribuidos un conjunto de instancias donde las instancias de cada uno de los conjuntos son
muy similares entre sÃ­ y diferentes a las instancias asignadas a los otros conjuntos. Para conocer
si una determinada instancia pertenece o no a un grupo, se utiliza una media similitud en base a
la distancia (normalmente se utiliza la distancia euclidea) entre el elemento que representa al
cluster denominado centroide y el ejemplo. El centroide es un vector de caracterÃ­sticas que
representa el punto medio del cluster. Cada uno de estos conjuntos o â€œclusteresâ€ son las
diferentes clases en las que se agruparÃ¡n las instancias de entrenamiento y que definirÃ¡n las
futuras predicciones del modelo generado. De forma general, se puede decir que el objetivo de
este mÃ©todo consiste en aprender los valores de los atributos (variables) que representan a los
centrodiodes de cada uno de los grupos o clÃºsteres en los cuales se distribuirÃ¡n las instancias
de entrenamiento. Este algoritmo se aplica sobre conjuntos de instancias con atributos
(variables) de tipo continuo, debido a que la mayor parte de las funciones de distancias utilizadas
para medir la pertenencia de una instancia a un cluster estÃ¡n basadas en la distancia euclidea o
algÃºn otro tipo de distancia de tipo espacial. En el caso de que existan atributos de tipo discreto
estos deben ser transformados en variables de tipo continuo con el fin de poder aplicar la funciÃ³n
de distancia. En la Figura se presenta un ejemplo de un posible modelo construido mediante la
utilizaciÃ³n del algoritmo K-medias. En este caso se ha seleccionado un valor de k=3, lo que
producirÃ¡ que el nÃºmero de clusters generados por el algoritmo sea tres utilizando un conjunto
de instancias de entrenamiento compuestas por dos atributos de tipo continuo.

6Inteligencia artificial â€“ Apredizaje automÃ¡tico II
Figura 1: Ejemplo de modelo generado por el algoritmo K-medias utilizando un valor de K igual 3.
2.1.1. Funcionamiento del proceso de auto organizaciÃ³n
Como indicamos anteriormente el proceso de aprendizaje del algoritmos K-medias consistente
en calcular los valores de los atributos de los centroides que representan a cada uno de los
clÃºsteres. Es decir el modelo generado por este algoritmo consiste en un conjunto de vectores
cada uno de los cuales corresponde con uno de los centroides. Cada vez que se quiera predecir
la clase de una nueva instancia en base a este modelo se calcularÃ¡ la distancia a cada uno de
los centroides seleccionÃ¡ndose la clase que representan al centroide que se encuentra mÃ¡s
cerca. El proceso de aprendizaje es un proceso de refinamiento iterativo debido a que el valor
de los centroides se va refinando en base a las instancias de entrenamiento. Este proceso se
encuentra dividido en tres fases:
â–ª
InicializaciÃ³n: El algoritmo K-medias no es capaz de seleccionar de forma automÃ¡tica el
nÃºmero de clÃºsteres en los cuales se deben dividir las diferentes instancias, por lo que
es necesario indicar al algoritmo cual serÃ¡ el nÃºmero de clÃºsteres en los cuales deben
dividirse las instancias. Una vez definido este valor, se utiliza un algoritmo de
particionamiento aleatorio [2] mediante el cual se seleccionan de forma aleatoria k
observaciones del conjunto de las instancias de entradas que son utilizados como
7
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
centroides iniciales. La distribuciÃ³n de los centroides da lugar a un diagrama de Vorononi
[3]. Esta fase sÃ³lo se realiza al comienzo del algoritmos el cual ejecutarÃ¡ las dos fases
siguientes durante una serie de iteraciones.
â–ª
AsignaciÃ³n: A continuaciÃ³n se realiza un proceso de asignaciÃ³n de las diferentes
instancias del conjunto de entrenamiento a su correspondiente cluster. Para ello se
calcula la distancia entre cada instancia y los centroides de cada cluster asignÃ¡ndose la
instancia al cluster cuyo centroide estÃ¡ mÃ¡s cerca. El cÃ¡lculo de la distancia suele
realizarse mediante la utilizaciÃ³n de la distancia euclidea pero es posible utilizar otra
medida de distancia.
â–ª
ActualizaciÃ³n: Una vez realizara la asignaciÃ³n de las instancias a cada grupo se produce
una actualizaciÃ³n de los valores del centroide en base a las diferentes instancias que han
sido asignadas al cluster. Los nuevos valores del cluster se calculan en base a la siguiente
ecuaciÃ³n
ğ‘ğ‘–ğ‘¡+1 =
1
âˆ‘ ğ‘¥ğ‘—
|ğ‘†ğ‘–ğ‘¡ |
ğ‘¡
ğ‘¥ğ‘— âˆˆ ğ‘†ğ‘–
donde i es el Ã­ndice del centroide que se estÃ¡ recalculando en el instante de tiempo t+1,
ğ‘†ğ‘–ğ‘¡ es el conjunto de instancias que pertenecen al centroide i en el instante t de tiempo y
ğ‘¥ğ‘— representa un determinad ejemplo del conjunto ğ‘†ğ‘–ğ‘¡ .
Las fases de asignaciÃ³n y actualizaciÃ³n se ejecutarÃ¡n hasta que cada uno de los centroides
converja a un Ã³ptimo global. Es decir, hasta que no se produzca ninguna actualizaciÃ³n en
ninguno de los centroides en base a las diferentes instancias del conjunto de entrenamiento.
Debido a que el algoritmo k-medias es un algoritmo heurÃ­stico, guiado por una medida de
distancia, no existen ningÃºn tipo de garantÃ­a de que se converja a un Ã³ptimo local que provocara
que se finalizara el proceso de aprendizaje. Con el fin de evitar este problema, se introduce una
conducciÃ³n de parada mediante un nÃºmero mÃ¡ximo de iteraciones de las fases asignaciÃ³n y
actualizaciÃ³n con el fin de parar el proceso de aprendizaje en caso de que el algoritmo no sea
capaz de converger.

8Inteligencia artificial â€“ Apredizaje automÃ¡tico II
2.2 Mapas autoorganizativos
Los mapas auto-organizados (Self-Organizing Maps, SOM en sus siglas en inglÃ©s) o mapas de
auto-organizativos de Kohonen [4] es un mÃ©todo de aprendizaje no supervisado de tipo
competitivo que construye una red de neuronas artificial con una estructura especÃ­fica. Este
mÃ©todo de aprendizaje normalmente es considerado como un algoritmo de agrupamiento ya que
agrupa la informaciÃ³n en clases en base a sus caracterÃ­stica, aunque algunos investigadores lo
consideran como un algoritmo de clasificaciÃ³n aunque las clases no se conozcan a priori. El
proceso de construcciÃ³n de esta red no utiliza ningÃºn tipo de conocimiento de control que indique
si la red neuronas se estÃ¡n construyendo (configurando) de forma correcta debido a que las
instancias del conjunto de entrenamiento no poseen ningÃºn tipo de informaciÃ³n objetivo (salida)
que indique como deberÃ­a configurarse a la red. La red auto-organizada debe descubrir
caracterÃ­sticas similares, regularidades, correlaciones o categorÃ­as en las instancias de entrada
que serÃ¡n incluidos a la estructura interna de conexiones de las diferentes neuronas. Este
proceso de auto-organizaciÃ³n de las conexiones entre las neuronas en funciÃ³n de los estÃ­mulos
procedentes de la informaciÃ³n de las instancias del conjunto de entrenamiento es el que da
nombre a la tÃ©cnica. En el proceso de aprendizaje competitivo las neuronas compiten unas con
otras con el fin de realizar una determinada tarea. Es decir cuando se introduce un valor de
entrada a la red sÃ³lo una de las neuronas de salida (o un grupo de ellas localizadas
espacialmente en la misma Ã¡rea de la cuadricula que define la salida) se deben activar. Este
comportamiento produce que las neuronas compitan por activarse, quedando finalmente una
Ãºnica neurona como vencedora y anulÃ¡ndose el resto de ellas, las cuales son forzadas a sus
valores de respuesta mÃ­nimos. El objetivo de este proceso de aprendizaje es categorizar los
datos que se introducen en la red de forma que cuando se detectan valores similares en la misma
categorÃ­a se debe activar la misma neurona de salida. Las diferentes clases o salidas deben ser
definidas por la propia red mediante las correlaciones entre los datos de entrada. Es decir, los
mapas auto-organizativos se pueden definir como un algoritmos para clasificar las diferentes
observaciones que existen en el conjunto de entrenamiento.
2.2.1. Estructura de los mapas auto-organizativos
El modelo resultante de esta tÃ©cnica consiste en una red de neuronas compuesta por dos capas
cuyas conexiones son hacia delante. La capa de entrada compuesta por N neuronas, una por
cada caracterÃ­stica de las instancias del conjunto de entrenamiento, encargada de transmitir la
informaciÃ³n de entrada a la capa de salida. La capa de salida compuesta por M neuronas es la
encargada de procesar la informaciÃ³n y formar el mapa de categorÃ­as. La estructura de la capa
9
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
de salida estÃ¡ distribuida como una cuadrÃ­cula o mapa bidimensional, donde las diferentes
neuronas de la cuadrÃ­cula representan a cada categorÃ­a (pesos, segÃºn la notaciÃ³n de Kohonen)
estÃ©n correlacionados espacialmente, de modo que los puntos mÃ¡s prÃ³ximos en la rejilla sean
mÃ¡s parecidos entre sÃ­ que los que estÃ©n muy separados. La estructura de capa de salida suele
corresponderse con rectÃ¡ngulo o con un hexÃ¡gono.
Figura 2: Estructura de la red de neuronas de un mapa auto-organizativo para cuatro posibles clases de salida
La Figura 2 presenta una ejemplo de un mapa auto-organizativo donde tenemos la capa de
entrada estÃ¡ formada por tres neuronas que se corresponden con las tres caracterÃ­sticas
presentes en la instancias de entrada y la capa de salida formada por una cuadrÃ­cula de 9x7
neuronas que son capaces de predecir 4 clases diferentes. Cada una de las neuronas de la capa
de entrada estÃ¡ conectada con cada una de las neuronas de la capa de salida mediante un
conexiÃ³n que posee un peso. De manera que cada una de las neuronas de la capa de salida
tienen asociado un vector de pesos, denominado vector de referencia, en el cual se almacenan
los valores aproximados de las caracterÃ­sticas que identifican la categorÃ­as representadas por la
neurona de salida. Esto hace que exista algÃºn tipo de conexiÃ³n implÃ­cita de excitaciÃ³n e inhibiciÃ³n
(vecindad), aunque no estÃ©n realmente conectadas, entre las neuronas que estÃ¡n juntas debido
a que los valores del vector de referencia son muy similares.

10Inteligencia artificial â€“ Apredizaje automÃ¡tico II
2.2.2. Funcionamiento del proceso de auto organizaciÃ³n
El proceso de aprendizaje de los mapas auto-organizativos consistente en calcular los valores
de cada uno de los vectores de las neuronas de la capa de salida en base a las instancias del
conjunto de entrenamiento. El proceso de aprendizaje estÃ¡ dividido en dos fases:
â–ª
InicializaciÃ³n: El proceso de inicializaciÃ³n comienza mediante la selecciÃ³n de forma
aleatoria de el nÃºmero de neuronas que formarÃ¡n parte de la capa de salida, aunque en
algunas implementaciones es posible configurar el tamaÃ±o. A continuaciÃ³n se inicializan
los valores de los vectores de referencia (pesos) de las neuronas del mapa. Este proceso
de inicializaciÃ³n se puede realizar mediante la generaciÃ³n de valores aleatorios o
mediante un muestreo aleatorio en base a los valores de dos instancias de entrada. El
segundo mÃ©todo de inicializaciÃ³n suele acelerar el proceso de â€œauto-organizaciÃ³nâ€ debido
a que los pesos iniciales son una aproximaciÃ³n de los posibles pesos que utilizarÃ¡ el
mapa.
â–ª
Entrenamiento: El proceso de entrenamiento consta a su vez de dos fases las cuales se
aplican sobre cada una de las instancias del conjunto de entrenamiento un nÃºmero de
iteraciones que debe ser definida a priori. La primera de estas dos fases consiste en
calcular la distancia de uno de los vectores de entrada, seleccionado de manera aleatoria,
a cada uno de los vectores de referencia de las neuronas de la capa de salida (mapa).
Normalmente se utiliza la distancia euclidea, pero puede utilizarse otra medida de
distancia en base a la naturaleza de los atributos de las diferentes instancias de
entrenamiento. A continuaciÃ³n se identifica la neurona mÃ¡s similar (Best Matching Unit,
BMU en sus siglas en inglÃ©s) a la instancia, en base a la medida de distancia utilizada, y
se realiza un proceso de actualizaciÃ³n de los pesos de todas la neuronas del mapa con
respecto a la neurona BMU. Normalmente sÃ³lo se producirÃ¡n cambios en la neurona BMU
y en aquella que se consideren cercanas, es decir aquellas que se encuentran en su
vecindad. Durante el proceso de aprendizaje se va produciendo un decremento de la tasa
de aprendizaje con el fin de conseguir que cada neurona se especialice y el ratio de
propagaciÃ³n de los cambios sea cada vez menor. Es decir, si se permiten una
propagaciÃ³n uniforme en el mapa durante el proceso de aprendizaje se perderÃ­a el
concepto de especializaciÃ³n de las neuronas del mapa eliminando la posibilidad de que
aparecieran clases muy residuales, de forma que el algoritmo incluye la tasa de
aprendizaje la cual van acercÃ¡ndose a cero a lo largo del proceso de aprendizaje
11
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
permitiendo la especializaciÃ³n de las diferentes neuronas del mapa. La formula utilizada
para realizar la actualizaciÃ³n de los pesos de los vectores de referencia es la siguiente:
ğ‘Šğ‘£ (ğ‘¡ + 1) = ğ‘Šğ‘£ (ğ‘¡) + ğ‘‰(ğ‘ğ‘šğ‘¢, ğ‘£, ğ‘ ) ğ›¼(ğ‘£)(ğ‘‡(t) - ğ‘Šğ‘£ )
Donde ğ‘Šğ‘£ (ğ‘¡) es el vector de pesos de la neurona que se quiere actualizar, t es el Ã­ndice
de la iteraciÃ³n en el proceso de aprendizaje, BMU es el Ã­ndice de la neurona BMU que ha
sido identificada como la mÃ¡s cercana al ejemplo de entrenamiento, ğ›¼(ğ‘£) es la tasa
monÃ³tonamente decreciente de aprendizaje, s es Ã­ndice, diferente de v, de todas las otras
neuronas del mapa y t es el Ã­ndice del ejemplo de entrenamiento en el conjunto T
(conjunto de entrenamiento).
Figura 3 - Ejemplo de funcionamiento del proceso de actualizaciÃ³n de los pesos del mapa. Â© Wikipedia
La funciÃ³n V es la funciÃ³n de vecindad que indica si las neuronas con Ã­ndice bmu y v
forman parte de la misma vecindad. En su versiÃ³n mÃ¡s sencilla esta funciÃ³n devuelve el
valor 1 a todas las neuronas suficientemente cerca a la neurona BMU y 0 a las otras, pero
es mÃ¡s comÃºn elegir una funciÃ³n gaussiana que define de forma mÃ¡s precisa el
distanciamiento que se va produciendo entre la neuronas. Observando la formula se
puede apreciar que segÃºn se va produciendo el distanciamiento de las neuronas el ratio
de cambio en el valor de los pesos es menor produciendo la especializaciÃ³n de las
neuronas hacia las diferentes clases. Este proceso de distancia se produce en base a la
tasa de aprendizaje que va acercÃ¡ndose a 0 y al resultado de la funciÃ³n de distancia. En
la Figura 3 se presenta la forma en la cual se va auto-organizando el mapa durante el
proceso de aprendizaje, se puede considerar que el mapa es como una red elÃ¡stica que
se va plegando dentro del espacio de las variables que representan las caracterÃ­sticas de
las instancias, por lo que aunque la topologÃ­a de la capa de salida sea representada

12Inteligencia artificial â€“ Apredizaje automÃ¡tico II
mediante un rectÃ¡ngulo o un hexÃ¡gono la estructura real mapa depende del proceso de
aprendizaje y se parece mÃ¡s a la estructura de la Figura 3.
3. APRENDIZAJE PROFUNDO
El Aprendizaje Profundo o Deep Learning es un termino que apareciÃ³ por primera vez en
2006[12] para describir un conjunto de nuevas arquitecturas de redes de neuronas artificiales
que eran capaces de aprender mucho mejor que las arquitecturas tradicionales o planas. Este
nuevo tipo de redes de neuronas se diferencian principalmente de las arquitecturas tradicionales
en la utilizaciÃ³n de un elevado nÃºmero de capas de tipo oculto que aplican diferentes tipo de
operaciones no lineales que permiten transformar la informaciÃ³n con el fin de extraer nuevas
caracterÃ­sticas. AdemÃ¡s la utilizaciÃ³n de un nÃºmero tan elevado de capas permite representar la
informaciÃ³n utilizando diferentes niveles de abstracciÃ³n simplificando el proceso de clasificaciÃ³n
y/o permitiendo la creaciÃ³n de nuevas caracterÃ­sticas que permiten aprender ciertos conceptos
imposibles para las arquitecturas tradicionales. Este nuevo tipo de arquitecturas basadas en la
utilizaciÃ³n de mÃºltiples combinaciones de capas supuso un salto cualitativo en la creaciÃ³n de
modelos que eran capaces de manejar conjuntos de datos con una elevadÃ­sima dimensionalidad
y/o representaciones paramÃ©tricas no lineales. Debido a este tipo de arquitecturas comenzÃ³ a
ser posible identificar, de manera hasta cierto punto eficiente, objetos, personas en base a
caracterÃ­sticas muy especÃ­ficas mediante la utilizaciÃ³n de imÃ¡genes en bruto (redes de neuronas
profundas convolucionales) o procesar lenguaje tanto hablado como escrito con el fin de crear
sistemas de traducciÃ³n en tiempo real (redes de neuronas profundas recurrentes). A continuaciÃ³n
se describe de forma detallada el funcionamiento de las redes de neuronas convoluciones las
cuales son probablemente el tipo de red de neuronas de Aprendizaje Profundo mÃ¡s utilizadas.
3.1 Redes de neuronas convolucionales
Una red de neuronas convolucional (Convolutional Neural Networks CNN, en sus siglas en
inglÃ©s) es un tipo de red de neuronas artificial que permite construir modelos mediante la
utilizaciÃ³n de aprendizaje supervisado. Este tipo de red de neuronas intenta imitar la estructura
y funcionamiento del cortex visual del ojo humano con el fin de poder extraer informaciÃ³n de la
imÃ¡genes en bruto y conseguir detectar objetos, personas, etc. Para ellos las redes
convolucionales utilizan diferentes tipos de capas ocultas especializadas en identificar ciertos
tipos de informaciÃ³n siguiente un flujo especÃ­fico en el tratamiento de la informaciÃ³n desde
caracterÃ­sticas mÃ¡s generales acerca de la imagen (lÃ­nea, bordes, colores, etc.) hasta
13
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
caracterÃ­sticas mÃ¡s especÃ­ficas (rasgos faciales, caras, patrones en el rostro, etc.) construidas
por combinaciÃ³n de las mÃ¡s generales. En la Figura 4 se presenta un ejemplo de las diferentes
capas de una red profunda de neuronas convolucional formada por 19 capas donde se aplican
diferentes operaciones que nos permiten extraer caracterÃ­sticas de las imÃ¡genes insertadas
como entrada y utilizar todas estÃ¡s caracterÃ­sticas para construir una clasificaciÃ³n utilizando las
dos ultimas capas denominadas Fully Connected (FD). En una red convolucional se pueden
identificar dos procesos: (1) uno de extracciÃ³n de caracterÃ­sticas; y (2) otro de clasificaciÃ³n.
Figura 4: DistribuciÃ³n de las diferentes capas de una red de neuronas convolucional
3.1.1. ExtracciÃ³n de caracterÃ­sticas
El proceso de extracciÃ³n de caracterÃ­sticas permite extraer informaciÃ³n de la imÃ¡genes
comenzando por la extracciÃ³n de informaciÃ³n general que es comÃºn a cualquier tipo de imagen,
como por ejemplo lÃ­neas, colores, trazos, bordes hasta caracterÃ­sticas mÃ¡s especÃ­ficas que estÃ¡n
mÃ¡s cercanas a los diferentes elementos que queremos identificar en las imÃ¡genes, como por
ejemplo, caras, formas especÃ­ficas, sÃ­mbolos, etc. Para poder extraer este tipo de informaciÃ³n se
introducen en la red una serie de capas que permiten extraer este tipo de informaciÃ³n mediante
la aplicaciÃ³n de operaciones matemÃ¡ticas las cuales permiten focalizarse en el proceso de
extracciÃ³n con el fin de identificar ciertos patrones especÃ­ficos. Las capas mÃ¡s comunes
utilizadas en una red de neuronas profunda de tipo convolucional son las capas convoluciones y
de subsampling.
RepresentaciÃ³n de la informaciÃ³n
La primera etapa del proceso consiste en transformar una imagen de entrada en neuronas de la
capa entrada para ello se utiliza la estructura en forma de matriz de pÃ­xeles de la propia imagen.

14Inteligencia artificial â€“ Apredizaje automÃ¡tico II
Es decir, cada pixel de la imagen se asigna a una neurona de entrada. Para la explicaciÃ³n de los
diferentes procesos vamos a utilizar una imagen de 7x7x1, es decir, nuestra imagen tendrÃ¡ 49
pÃ­xeles y un canal de color (escalada de grises) lo que supondrÃ­a que tendrÃ­amos 49 neuronas
en la capa de entrada. En el caso de que estuviÃ©ramos utilizando una imagen en color con tres
canales (Rojo, Verde y Azul) tendrÃ­amos 7x7x3 = 147 neuronas en la capa de entrada. En la
Figura 5 se presenta como las imÃ¡genes en color se dividen en tres imÃ¡genes cada una
correspondiente a uno de los tres canales del RGB. Cuando se utilizan este tipo de imÃ¡genes
como entrada de la red de neuronas el nÃºmero de neuronas de la capa e entrada debe
multiplicarse por tres.
Figura 5: Ejemplo de cÃ³mo una imagen se divide de las imÃ¡genes
correspondientes a los tres canales de color (RGB)
Capa convolucional
Las capas convolucionales son las capas que producen la verdadera extracciÃ³n de
caracterÃ­sticas en este tipo de redes. Para ellos se utilizan operaciones de tipo matricial,
denominadas convoluciones, que producen modificaciones en los pÃ­xeles con el fin de resaltar
cierto tipo de informaciÃ³n en la imagen con el fin de extraer nuevas caracterÃ­sticas. Una
convoluciÃ³n es una transformaciÃ³n de tipo local que se aplica sobre una matriz de pixeles donde
el valor del pixel resultantes tras la aplicaciÃ³n de la convoluciÃ³n depende de la vecindad local de
cada pixel, la cual viene definida por el filtro (kernel) que se utiliza para definir la vecindad y
producir la convoluciÃ³n.
15
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
Figura 6: Ejemplo de funcionamiento de un convoluciÃ³n aplicada mediante un kernel 3x3.
En la Figura 6 se presenta un ejemplo de un proceso de convoluciÃ³n sobre una imagen a la que
se le aplica un filtro de tamaÃ±o 3x3. Este proceso consiste en transformar el valor de cada uno
de los pÃ­xeles mediante la aplicaciÃ³n de un filtro que realiza un producto escalar. Esta operaciÃ³n
se aplica sobre todas las neuronas de entrada con el fin de generar una nueva matriz de salida.
Aunque realmente las capas de convoluciÃ³n no sÃ³lo aplican un Ãºnico filtro sobre su neuronas de
entrada sino que aplican un conjunto de filtros que producen la generaciÃ³n en un conjunto de
imÃ¡genes cada una de las cuales ha sufrido un proceso mediante el cual se han resaltados
ciertos aspectos de la imagen. Es decir, si tuviÃ©ramos una imagen de 7x7x1 (ancho, alto y canal)
y aplicÃ¡ramos un conjunto de 32 filtros generarÃ­amos 32 nuevas imÃ¡genes que supondrÃ­an 1568
neuronas en la siguiente capa oculta. AdemÃ¡s este ejemplo se ha descrito sobre sÃ³lo un canal.
Las imÃ¡genes estÃ¡n formadas por tres canales (RGB) de forma que al aplicar un filtro deberÃ­amos
aplicarlo sobre los tres canales de la imagen de forma que el filtro 3x3 de la Figura 6 se
convertirÃ­a en un filtro 3x3x3.
Capa de subsampling
Las capas convolucionales incrementan el nÃºmero de neuronas debido al proceso de resaltado
que
provocan
los
filtros,
si
continuÃ¡ramos
incluyendo
mÃ¡s
capa
convolucionales
incrementariamos el nÃºmero de neuronas hasta un punto que serÃ­a incontrolable, por lo que con
el fin de reducir el nÃºmero de neuronas tras la aplicaciÃ³n de varias capas convolucionales se
suele aplicar una capa de subsampling. Este tipo de capa intenta reducir el nÃºmero de neuronas
mediante un proceso de combinaciÃ³nes que intentan mantener las caracterÃ­sticas que han sido

16Inteligencia artificial â€“ Apredizaje automÃ¡tico II
resaltadas durante las convoluciones pero utilizando un nÃºmero menor de neuronas. Las capa
de subsampling mÃ¡s utilizadas son la cada de tipo Pooling (max, mediun, etc.).
Figura 7: Ejemplo de funcionamiento de un operaciÃ³n de max-pooling aplicada mediante un kernel 2x2.
En la figura se presentan una operaciÃ³n de Max-polling mediante la utilizaciÃ³n de un filtro de 2x2.
Este filtro se aplica sobre todas las matrices (imÃ¡genes) generadas en la capa anterior
transformando 4 neuronas en una. En el ejemplo se aplica una filtro de tipo Max que selecciona
aquel pixel que tiene el mayor valor. Este proceso de convoluciÃ³n y sampling se suele repetir
mÃºltiples veces con el fin de extraer la mayor cantidad de la imagen.
3.1.2. ClasificaciÃ³n
Tras el proceso de extracciÃ³n de caracterÃ­sticas se produce un aplanamiento de la informaciÃ³n
que transforma las neuronas de la Ãºltima capa de extracciÃ³n en una Ãºnica capa de neuronas
mediante una capa de tipo Fully Connected la cual puede estar conectada a otro tipo de capa o
directamente a la salida mediante una capa de tipo Softmax que se usa normalmente para
conectar la Ãºltima capa oculta con la capa de salida cuyo nÃºmero de neuronas serÃ¡ igual al
nÃºmero de clases entre las cuales estamos clasificando. En la Figura 8 se presenta el
funcionamiento completo de una red convolucional formada por los dos procesos descritos
previamente.
17
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
Figura 8: Funcionamiento completo de una red convolucional para
la clasificaciÃ³n de imÃ¡genes de diferente tipos de vehÃ­culos
4. APRENDIZAJE POR REFUERZO
El aprendizaje por refuerzo [5] [13] es una de las familias del aprendizaje automÃ¡tico basado en
el aprendizaje conductista que permite construir modelos de aprendizaje basados en polÃ­ticas.
Es decir, este tipo de algoritmos tratan de identificar la secuencia de acciones que debe escoger
un agente en base a la informaciÃ³n del entorno maximizando algÃºn tipo de recompensa que se
obtendrÃ¡ tras la aplicaciÃ³n de las acciones. Es decir, este funcionamiento recuerda el proceso
de aprendizaje que utilizamos lo seres vivos para aprender o enseÃ±ar ciertos conceptos
relacionados con la interacciÃ³n con el entorno de manera automÃ¡tica. Como por ejemplo, cuando
los humanos aprendemos a caminar de manera bÃ­peda donde recibimos un â€œrefuerzo negativoâ€
cuando nos caemos debido a que no hemos conseguido alcanzar nuestra posiciÃ³n objetivo, o en
el peor caso nos hemos caÃ­do y hemos sufrido un golpe, o un â€œrefuerzo positivoâ€ cuando hemos
sido capaces de movernos correctamente hasta la posiciÃ³n objetivo. El proceso de aprendizaje
va adaptando nuestro organismo de forma que seamos capaces de mantener el equilibrio y
mover de forma correcta las articulaciones del cuerpo para desplazarnos de la forma correcta.
Como se puede observar el modo de funcionamiento de esta familia de mÃ©todos de aprendizaje
difiere de los mÃ©todos de tipo supervisado y no supervisados a los cuales se les proporciona
todo el conjunto de instancias de entrenamiento a priori con el fin de construir el modelo. En
cambio los mÃ©todos de aprendizaje por refuerzo obtienen las diferentes instancias de
entrenamiento de manera activa interactuando con el entorno en diferentes iteraciones. En la
Figura 9 se presenta un diagrama que describe el funcionamiento bÃ¡sico de un sistema basado

18Inteligencia artificial â€“ Apredizaje automÃ¡tico II
en Aprendizaje por Refuerzo el cual estÃ¡ formado por dos entidades bÃ¡sicas, el agente y el
entorno, que interactÃºan entre sÃ­ por medio de las acciones que ejecuta el agente en el entorno.
â–ª
Agente: El agente es la entidad principal del proceso de Aprendizaje por Refuerzo que
interactua con el entorno obteniendo informaciÃ³n y produciendo variaciones en el
mediante la aplicaciÃ³n de acciones.
â–ª
AcciÃ³n: Es la acciÃ³n que es ejecutada por el agente en el entorno con el fin de producir
una variaciÃ³n en el. Las lista de posibles acciones depende del estado del entorno, debido
a que no es posible aplicar cualquier acciÃ³n en cualquier estado. En el caso de un
videojuego bidimensional como el Pacman, la lista de posibles acciones puede incluir
moverse hacia la derecha, hacia la izquierda, hacia arriba, hacia abajo o quedarse quieto.
Figura 9: Arquitectura bÃ¡sica de un proceso de Aprendizaje por Refuerzo
â–ª
Estado: El estado es la representaciÃ³n completa del entorno en un instante especÃ­fico de
tiempo. Esta informaciÃ³n incluye toda la informaciÃ³n referente al entorno, asÃ­ como la
informaciÃ³n que representa al agente.
â–ª
Refuerzo: El refuerzo es un valor numÃ©rico de tipo real que se obtiene tras la ejecuciÃ³n
de una acciÃ³n en el entorno. Este valor es utilizado como medida para evaluar si la
ejecuciÃ³n de una acciÃ³n ha sido positiva o negativa para el agente dependiendo del valor
de refuerzo obtenido.
â–ª
Entorno: Es la representaciÃ³n virtual del mundo en la cual interactua el agente. El entorno
almacena el estado actual el cual es modificado por las acciones (entradad) ejecutadas
19
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
por el agente generando a continuaciÃ³n como salida el nuevo estado del entorno y la
recompensa obtenida tras la ejecuciÃ³n de la acciÃ³n.
El objetivo de un proceso de aprendizaje por refuerzo consiste en resolver un problema de
decisiÃ³n secuencial que implica la selecciÃ³n de un conjunto de acciones (decisiones) cuyo
resultado (refuerzo) no se conoce hasta el final. Para ello el sistema de aprendizaje deberÃ¡
construir un modelo en forma de polÃ­tica Ï€(s, a) que deberÃ¡ sugerir la mejor acciÃ³n a ejecutar en
el entorno. El cual suele ser modelado mediante un Proceso de DecisiÃ³n de Markov [6] (Markov
Decission Process, MDP en sus siglas en inglÃ©s) completamente observable si es posible obtener
informaciÃ³n completa del entorno, pero existe algÃºn tipo de incertidumbre asociada al resultado
de la acciÃ³n, es decir existe una probabilidad de que la acciÃ³n no se ejecute de manera correcta
y produzca un estado diferente al esperado. Aunque existe la posibilidad de que el entorno sea
parcialmente observable, lo que producirÃ¡ un Proceso de DecisiÃ³n de Markov Parcialmente
Observable [7] (Partially Observable Markov Decission Process, POMDP en sus siglas en inglÃ©s)
cuando ademÃ¡s de incertidumbre en la ejecuciÃ³n de las acciones existe tambiÃ©n incertidumbre
en la informaciÃ³n referente a los estados, es decir el agente no es capaz de observar de manera
completa el estado, sÃ³lo es capaz de obtener informaciÃ³n parcial.
4.1 Procesos de decisiÃ³n de Markov
Un proceso de decisiÃ³n de Markov (Markov Decision Process, MDP en sus siglas en inglÃ©s) es
un tipo de problema de decisiÃ³n secuencial aleatoria dependiente del tiempo en el cual se cumple
la propiedad de Markov [8]. Este propiedad define que las recompensas que se pueden obtener
en el futuro mediante la aplicaciÃ³n de acciones en un determinado estado sÃ³lo depende de dicho
estado y no de los anteriores. Es decir, los refuerzos que vayamos a obtener tras la aplicaciÃ³n
de las diferentes acciones disponibles para un estado sÃ³lo dependerÃ¡n del estado actual del
entorno y no de los estados anteriores por los que se ha transitado hasta llegar al estado actual.
Un proceso de decisiÃ³n de Markov se puede definir de manera formal como una tupla MDP =
(S, A, T, R):
â–ª
S es el conjunto finito de estados, donde ğ‘ ğ‘¡ âˆˆ ğ‘† es el estado en el que se encuentra el
agente en el instante temporal t. De manera formal el conjunto de estados S del entorno
se define como un conjunto finito ğ‘† = {ğ‘ 1 , ğ‘ 2 , . . . , ğ‘ ğ‘› }, siendo n el tamaÃ±o del espacio de
estados. La forma en la que se modela el entorno depende de cada problema, por lo que
no hay una representaciÃ³n especÃ­fica como ocurrÃ­a con el aprendizaje supervisado y no
Â© Structuralia
20Inteligencia artificial â€“ Apredizaje automÃ¡tico II
supervisado donde la informaciÃ³n se modelada como un vector de atributos. En cambio
cuando aplicamos aprendizaje por refuerzo podemos modelar la informaciÃ³n de la
manera que mÃ¡s se adapta al problema a resolver. Normalmente la informaciÃ³n suele
modelarse como una matriz multidimensional ya que muchos de los problemas tratados
consisten en aplicar una serie de acciones en un entorno multidimensional.
â–ª
A es un conjunto finito de acciones, cada una de las cuales sÃ³lo puede ser aplicada en
un determinado estado. Es decir sea ğ‘ğ‘¡ âˆˆ ğ´(ğ‘ ğ‘¡ ) el conjunto de acciones que puede ser
aplicadas en el estado ğ‘ ğ‘¡ . Es posible que existan estados en los cuales no es posible
aplicar ningÃºn tipo de acciones, siendo estos estados considerados como sumideros o
estados muertos (dead-end), ya que una vez alcanzados es imposible resolver el
problema.
â–ª
La funciÃ³n ğ‘‡(ğ‘ ğ‘¡ , ğ‘, ğ‘ ğ‘¡+1 ) es una funciÃ³n de probabilidad tal que T: S x A â†’ P(S) que define
la probabilidad de que se produzca una transiciÃ³n del estado ğ‘ ğ‘¡ al estado ğ‘ ğ‘¡+1 tras las
aplicaciÃ³n de la acciÃ³n ğ‘ğ‘¡ .
â–ª
R es una funciÃ³n de refuerzo tal que R: S x A donde R(ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) es el refuerzo recibido tras
ejecutar la acciÃ³n ğ‘ğ‘¡ en el estado ğ‘ ğ‘¡ .
Las funciones de transiciÃ³n y recompensa son los elementos que define el modelo del MDP. Un
MDP puede ser representado mediante un diagrama de transiciÃ³n de estados donde cada nodo
se corresponde con un estado y los arcos (dirigidos) denotan las diferentes acciones que
permiten transitar de un estado a otro. Esta representaciÃ³n grÃ¡fica normalmente incluye
informaciÃ³n referente a la recompensa que se obtiene al transitar a un estado y de la probabilidad
de cada transiciÃ³n. En la Figura 10 se presenta un ejemplo del diagrama de un MDP. Cada uno
de los arcos muestra el identificador de la acciÃ³n que produce la transiciÃ³n y dos valores
numÃ©ricos. El primero representa el refuerzo
que se obtiene al transitar de un estado a otro
utilizando esa transiciÃ³n y el segundo representa la probabilidad de que se produzca la transiciÃ³n.
Por el ejemplo, la aplicaciÃ³n de la acciÃ³n a1 en estado S2 da lugar a dos transiciÃ³nes donde la
probabilidad de ambas tiene que ser uno. En este caso con probabilidad igual a p = 0,2 (20%) se
transitarÃ­a al estado S0 y con probabilidad p = 0,8 (80%) se transitarÃ­a al estado S1. Normalmente
los MDP poseen al menos un estado denominado terminal en su conjunto de estados que
normalmente es utilizado como meta u objetivo de forma que se pueda resolver el problema que
representa el modelo.
21
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
Figura 10: RepresentaciÃ³n grÃ¡fica de un Proceso de DecisiÃ³n de Markov no determinista
La soluciÃ³n a un MDP suele consistir en identificar la polÃ­tica que permite alcanzar uno de los
posibles estados terminales o metas de manera optima. De forma que el objetivo en este tipo de
modelos consiste en identificar dicha polÃ­tica. Una polÃ­tica es una funciÃ³n ğœ‹ âˆ¶ ğ‘† ğ‘¥ ğ´ que realiza un
mapeo entre los estados y las acciones mediante sus distribuciones de probabilidad.
4.2 Funciones de valor
El objetivo al intentar resolver un problema modelado mediante un MPD consiste en encontrar la
polÃ­tica Ã³ptima o aquella que se encuentre lo mÃ¡s cerca posible de esta. Para conseguir encontrar
la polÃ­tica Ã³ptima es necesario definir algÃºn tipo de funciÃ³n que permita estimar como la calidad
de los estados desde la perspectiva del agente, es decir, necesitamos algÃºn tipo de medida que
nos permita medir como de bueno es para el agente llegar a un determinado estado en el entorno.
Para calcula ese valor de calidad es posible utilizar dos funciones:
â–ª
La funciÃ³n Q, denominada funciÃ³n de valor estado-acciÃ³n, que estima como de bueno es
aplicar una acciÃ³n en un determinado estado. De manera formal se puede definir la
funciÃ³n Q de una polÃ­tica ğœ‹, tal que ğ‘„ ğœ‹ âˆ¶ ğ‘† Ã— ğ´ â†’ ğ‘…, es igual a la recompensa obtenida
cuando a partir de un determinado estado s y una determinada acciÃ³n a, se sigue la
polÃ­tica ğœ‹:

22Inteligencia artificial â€“ Apredizaje automÃ¡tico II
âˆ
ğ‘„
ğœ‹ (ğ‘ ,
ğ‘) = âˆ‘ ğ›¾ ğ‘– ğ‘…(ğ‘ ğ‘– , ğ‘ğ‘– ) = ğ‘…(ğ‘ 0 , ğ‘0 ) + ğ›¾ ğ‘…(ğ‘ 1 , ğ‘1 ) + ğ›¾ 2 ğ‘…(ğ‘ 2 , ğ‘2 ) + â‹¯
ğ‘–=0
Esta ecuaciÃ³n puede ser refinada con el fin de obtener otra expresiÃ³n que es la que
realmente se utiliza en los diferentes algoritmos. De modo que la funciÃ³n Q puede ser
expresada tambiÃ©n en base a la siguiente ecuaciÃ³n:
ğ‘„ ğœ‹ (ğ‘ , ğ‘) = ğ‘…(ğ‘ , ğ‘) + ğ›¾ ğ‘„ ğœ‹ (ğ‘ â€², ğ‘â€²)
â–ª
La funciÃ³n V, denominada funciÃ³n de valor estado, estima como de bueno es que el
agente se encuentre en un determinado estado. De manera formal se puede definir la
funciÃ³n V de una polÃ­tica ğœ‹, tal que ğ‘‰ ğœ‹ âˆ¶ ğ‘† â†’ ğ‘…, es igual a la recompensa obtenida
cuando a partir de un determinado estado s se sigue la polÃ­tica ğœ‹:
âˆ
ğ‘‰
ğœ‹ (ğ‘ )
= âˆ‘ ğ›¾ ğ‘– ğ‘…(ğ‘ ğ‘– , ğœ‹(ğ‘ ğ‘– ))
ğ‘–=0
A pesar de cada una de esta funciones de valor se centra en un aspecto diferentes del modelo,
estas pueden ser relacionadas entre sÃ­ en base a la polÃ­tica que sigue, que es el elemento comÃºn
de ambas, mediante la siguiente ecuaciÃ³n:
ğ‘‰ ğœ‹ (ğ‘ ) = ğ‘„ ğœ‹ (ğ‘ , ğœ‹(ğ‘ ))
Ambas ecuaciones de valor nos permiten calcular una polÃ­tica Ã³ptima. Aunque en el caso de la
funciÃ³n de valor ğ‘‰ ğœ‹ es necesario conocer el modelo del MDP de forma completa, es decir es
necesario conocer las funciones de transiciÃ³n y de recompensa. Esto es debido a que la funciÃ³n
de valor ğ‘‰ ğœ‹ sÃ³lo contiene informaciÃ³n sobre la calidad de los estados, por lo que para poder
determinar la acciÃ³n Ã³ptima es necesario utilizar informaciÃ³n que es provista por las funciones
de transiciÃ³n y recompensa. En cambio, la funciÃ³n ğ‘„ ğœ‹ incluye informaciÃ³n sobre los estados y
las acciones lo que permite calcular la polÃ­tica Ã³ptima sin necesidad de conocer el modelo del
MDP. Debido a esta importante diferencia entre la funciones de Q y V hace que la mayorÃ­a de
23
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
los algoritmos de Aprendizaje por Refuerzo utilicen funciones de valor de estado-acciÃ³n debido
a que asumen que el modelo del MDP es total o parcialmente desconocido. A continuaciÃ³n
describiremos los primeros algoritmos que se desarrollaron para utilizar las funciones de valor y
calcular la polÃ­tica Ã³ptima del Modelo de DecisiÃ³n de Markov. Cabe destacar que en la
descripciÃ³n de estas funciones hemos asumido del el Modelo de DecisiÃ³n de Markov es
determinista, es decir las acciones que se aplican sobre los estados siempre producen la misma
transiciÃ³n. En el caso que necesitemos trabajar con sistemas no deterministas serÃ¡ necesarios
aplicar una serie de variaciones en las ecuaciones matemÃ¡ticas con el fin de poder controlar la
falta de determinismo en las acciones. Pero en este curso no vamos a tratar con MDPs no
deterministas. A continuaciÃ³n se presentan diferentes algoritmos que tratan de calcular la polÃ­tica
Ã³ptima en situaciones en las cuales tienen informaciÃ³n parcial o total del modelo descrito por el
MDP.
4.3 Algoritmos tradicionales
Uno de los principales problemas que existen a la hora de resolver un problema modelado
mediante un MDP o un POMDP es el conocimiento completo o parcial del modelo. Dependiendo
de que tipo de conocimiento se posea, el problema podrÃ¡ ser resuelto mediante la utilizaciÃ³n de
unas u otras tÃ©cnicas. En el caso de que se tenga un conocimiento completo del modelo, es
decir, se conocen la funciÃ³n de transiciÃ³n y la funciÃ³n de recompensa, es posible utilizar tÃ©cnicas
de programaciÃ³n dinÃ¡mica. Las tÃ©cnicas de programaciÃ³n dinÃ¡mica no suelen son considerados
como tÃ©cnicas de Aprendizaje por Refuerzo debido a que tiene conocimiento completo del
modelo y no necesitan aprenderlo, pero fueron el germen de los verdaderos algoritmos de
aprendizaje por refuerzo por lo que es necesario describir como funcionan para entender cuales
son las bases matemÃ¡ticas de los diferentes algoritmos de aprendizaje por refuerzo los cuales
se basan en la utilizaciÃ³n de funciones de valor de tipo Q. AdemÃ¡s los diferentes mÃ©todos que
serÃ¡n descritos a continuaciÃ³n asumen que los conjuntos de estados y acciones son finitos y lo
suficientemente pequeÃ±os para ser almacenados en algÃºn tipo de estructura de datos.
4.3.1. IteraciÃ³n de polÃ­ticas
El algoritmo de iteraciÃ³n de polÃ­ticas[9] (Policy Iteration, PI en sus siglas en inglÃ©s) es un
algoritmo de programaciÃ³n dinÃ¡mica que realiza variaciones en la polÃ­tica en cada iteraciÃ³n con
el fin de mejorarla hasta que la polÃ­tica converja. El funcionamiento de este algoritmo se basa en
tres fases:

24Inteligencia artificial â€“ Apredizaje automÃ¡tico II
â–ª
InicializaciÃ³n: En esta fase se produce la inicializaciÃ³n de las estructuras en las cuales se
almacena la polÃ­tica generÃ¡ndose una polÃ­tica aleatoria o basada en algÃºn tipo de
algoritmo de generaciÃ³n que minimiza el proceso de mejora de la polÃ­tica ya que parte de
una polÃ­tica mÃ¡s cercana a la polÃ­tica Ã³ptima. Esta fase sÃ³lo se ejecuta en la primera
iteraciÃ³n.
â–ª
EvaluaciÃ³n: Es esta fase se produce una evaluaciÃ³n de la polÃ­tica mediante el cÃ¡lculo de
la funciÃ³n de valor. Este proceso se denomina como predicciÃ³n de la funciÃ³n de valor y
para ello se utilizaciÃ³n la ecuaciÃ³n de Bellman.
â–ª
â–ª
ğœ‹ (ğ‘ ,
ğ‘„ğ‘¡+1
ğ‘) = ğ‘…(ğ‘ , ğ‘) + ğ›¾ ğ‘„ğ‘¡ğœ‹ (ğ‘‡(ğ‘ , ğ‘), ğœ‹(ğ‘‡(ğ‘ , ğ‘)))
Mejora: En esta fase se aplican una serie de variaciones en polÃ­tica con el fin de mejorarla
mediante la utilizaciÃ³n de la funciÃ³n de valor calculada en la fase anterior. Esta serie de
mejoras tratan de generar una nueva polÃ­tica con un comportamiento mÃ¡s cercano al de
la polÃ­tica Ã³ptima. Esta proceso suele realizarse mediante la utilizaciÃ³n de un algoritmo
avaricioso (greedy) que escoge para cada estado la acciÃ³n con mayor valor de la funciÃ³n
ğ‘„ ğœ‹ . Si consideramos que en la iteraciÃ³n anterior t se ha evaluado la polÃ­tica ğœ‹ğ‘¡ , la nueva
polÃ­tica mejorara se generarÃ­a en base a la siguiente expresiÃ³n:
ğœ‹ğ‘¡+1 (ğ‘ ) = arg max ğ‘„ ğœ‹ğ‘¡ (ğ‘ , ğ‘)
ğ‘
Si la nueva polÃ­tica generada mediante el proceso de mejora no presenta ninguna mejora
respecto a la anterior se considera que la polÃ­tica ha convergido y por lo tanto es la polÃ­tica
Ã³ptima. En caso contrario se almacena aquella polÃ­tica de entre las dos cuyo
comportamiento sea mÃ¡s similar a la polÃ­tica Ã³ptima y se vuelve a la fase dos del
algoritmo.
4.3.2. Value Iteration
El algoritmo de iteraciÃ³n de valor[10] (Value Iteration, VI en sus siglas en inglÃ©s) es un algoritmo
de programaciÃ³n dinÃ¡mica que realiza variaciones en la funciÃ³n de valor en cada iteraciÃ³n con
el fin de mejorarla hasta que encontrar la funciÃ³n de valor Ã³ptima. Mientras que el algoritmo PI
25
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
utilizaba la ecuaciÃ³n de Bellman para evaluar una polÃ­tica que posteriormente era mejorada, el
algoritmo VI utiliza la ecuaciÃ³n de optimalidad de Bellman para intentar calcular directamente la
funciÃ³n de valor Ã³ptima. El funcionamiento de este algoritmo se basa en tres fases:
â–ª
InicializaciÃ³n: En esta fase se produce la inicializaciÃ³n de las estructuras en las cuales se
almacena la funciÃ³n de valor generÃ¡ndose un funciÃ³n de valor aleatoria. Esta fase sÃ³lo
se ejecuta en la primera iteraciÃ³n.
â–ª
Mejora: En esta fase se aplican una serie de variaciones en la funciÃ³n de valor actual con
el fin de mejorarla mediante la utilizaciÃ³n ecuaciÃ³n de optimalidad de Bellman. Esta serie
de mejoras tratan de encontrar una nueva funciÃ³n de valor que estÃ¡ mÃ¡s cercana al
funciÃ³n Ã³ptima. Si consideramos que en la iteraciÃ³n anterior t se ha evaluado la polÃ­tica
ğœ‹ğ‘¡ , la nueva polÃ­tica mejorara se generarÃ­a en base a la siguiente expresiÃ³n:
ğ‘„ğ‘¡+1 (ğ‘ , ğ‘) = ğ‘…(ğ‘ , ğ‘) + ğ›¾ max ğ‘„ğ‘¡ (ğ‘‡(ğ‘ , ğ‘), ğ‘Â´)
ğ‘
4.4 Q-Learning
El algoritmo Q-Learning[14] es una algoritmo de Aprendizaje por Refuerzo que trata de aprender
una polÃ­tica mediante la utilizaciÃ³n de experiencias que son generadas mediante un proceso de
exploraciÃ³n del entorno. A diferencia de los algoritmos descritos anteriormente este no necesita
tener conocimiento completo del modelo, es decir no necesita conocer las probabilidades de
Ã©xito de las acciones. Las diferentes instancias de entrenamiento son experiencias las cuales se
representan como una tupla e = (s, a, sâ€™, r) donde: s es el estado actual del entorno; a es la acciÃ³n
que serÃ¡ aplicada sobre el entorno; sâ€™ es el estado resultante tras la aplicaciÃ³n de la acciÃ³n a; y
r es el refuerzo obtenido tras la aplicaciÃ³n de la acciÃ³n a en el estado s. Estas experiencias de
entrenamiento son utilizadas para construir la tabla Q que almacenada la polÃ­tica aprendiza
durante el proceso de entrenamiento.

26Inteligencia artificial â€“ Apredizaje automÃ¡tico II
Figura 11: Estado inicial de problema a resolver mediante el aprendizaje por refuerzo
Esta tabla es un mapa donde se almacena el mÃ¡ximo valor de refuerzo que puede ser obtenido
al aplicar una acciÃ³n en un estado. De forma sencilla esta tabla nos indica cual es la mejor acciÃ³n
que puede ser aplicada en cada estado. En la Figura 11 se presenta el estado inicial de un
posible entorno en el cual un robot se debe mover hasta la localizaciÃ³n denominada como meta.
Para ello el robot tendrÃ¡ que desplazarse por el entorno evitando las zonas de fuego y los muros
con el fin de llegar a la meta. En este caso el objetivo del proceso de aprendizaje es crear la tabla
Q que permita al robot alcanzar la meta sin caer en ninguna de las casillas que estÃ¡n ardiendo.
En este caso la tabla Q estÃ¡ formada por un conjunto de 30 estados que se corresponde con las
diferentes posiciones a las cuales se puede desplazar el robot y 4 posibles acciones que
permitirÃ¡n al robot moverse hacia arriba, abajo, izquierda y derecha, por lo que tendremos una
tabla de 30x4 posibles valores de refuerzo. Como se puede observar en la imagen, existen
estados en los cuales no es posible aplicar todas las acciones lo que supondrÃ¡ que el valor de
refuerzo de esos pares estado-acciÃ³n tendrÃ¡n un valor 0 y no deberÃ­an ser seleccionados nunca.
El proceso de aprendizaje
El proceso de aprendizaje del Algoritmo Q-Learning consiste en conjunto de episodios cada uno
de los cuales se corresponde con una ejecuciÃ³n completa del algoritmo con el fin de actualizar
el mayor nÃºmero posible de casillas de la tabla Q consiguien que los valores de esta converjan
de la manera mÃ¡s rÃ¡pida posible. Cada uno de estos episodios ejecuta un algoritmo de 5 fases:
â–ª
InicializaciÃ³n: Esta es la primera fase del algoritmo se inicializa la tabla Q, la tasa de
aprendizaje y el factor de descuento. Normalmente el valor de inicio es 0, pero existen
27
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
configuraciones en la cuales se asigna un valor aleatorio entre 0 y 1. AdemÃ¡s tambiÃ©n es
posible utilizar una tabla Q generada previamente con el fin de acelerar el proceso de
aprendizaje. Esta fase sÃ³lo se ejecuta una vez al inicio de cada episodio.
â–ª
SelecciÃ³n: Esta fase consiste en seleccionar la mejor acciÃ³n de la tabla Q y ejecutarla,
debido a que el comienzo del proceso de aprendizaje todas los pares estado-acciÃ³n
tienen valor 0 por lo que se produce una selecciÃ³n aleatoria de las acciones. Este proceso
de selecciÃ³n aleatoria es la que se conoce como proceso de exploraciÃ³n ya que el agente
realiza una exploraciÃ³n del entorno debido a que no tiene informaciÃ³n sobre el. A medida
que se va incluyendo informaciÃ³n sobre el entorno en la tabla Q, la exploraciÃ³n se
convierte en explotaciÃ³n debido a que comienza a explorarse sÃ³lo aquellas acciones que
tienen unos recompensas mayores.
â–ª
EjecuciÃ³n: En esta fase se realiza la ejecuciÃ³n de la acciÃ³n seleccionada en el entorno
generÃ¡ndose un nuevo estado como consecuencia de la ejecuciÃ³n de la acciÃ³n ejecutada
en estado previo.
â–ª
Calculo: En esta fase se realiza el calculo del nuevo valor de refuerzo mediante la funciÃ³n
de valor de tipo Q cuya informaciÃ³n es posteriormente almacenada en la tabla Q.
Figura 12: EcuaciÃ³n de actualizaciÃ³n de la tabla Q
1. Nuevo valor en la tabla Q para el estado y la acciÃ³n seleccionada.
2. Es el antiguo valor almacenado en la table Q para el estado y la seleccionada.
3. La tasa de aprendizaje (learning rate) es una variable cuyo valor estÃ¡ comprendido
entre 0 y 1 e indica cuando vamos a tener en cuenta lo que aprendamos en la nueva
experiencia producida por la ejecuciÃ³n de la acciÃ³n a en estado ğ‘ ğ‘¡ . Cuando su valor
es cercano a 1 significa que vamos a tener en cuenta las nuevas experiencias y
cuando el valor es cercano a 0, no se va a producir casi ningÃºn cambio en los valores
de la tabla Q, debido a que el agente no estÃ¡ aprendiendo sÃ³lo seleccionando
aquellas acciones con mejor valor en la tabla Q.
4. Refuerzo que se obtiene por ejecutar la acciÃ³n a en el estado st .

28Inteligencia artificial â€“ Apredizaje automÃ¡tico II
5. El factor de descuento (discount rate) es una variable cuyo valor estÃ¡ comprendido
entre 0 y 1 e indica la importancia de las acciones ejecutadas a largo plazo. Cuando
su valor es cercano a cero 0 el algoritmo sÃ³lo tiene en cuenta los refuerzo inmediatos,
en cambio si el valor es prÃ³ximo a 1 algoritmo se centra mÃ¡s en lo que pueda ocurrir
a largo plazo.
6. Es el maximo valor de recompensa que se obtendrÃ¡ al transitar al estado st+1. Este
valor se calcula como el maximo de todos los valores de recompensa que se
encuentran almacenados en la tabla Q para el estado st+1 .
â–ª
ActualizaciÃ³n: La fase actualizaciÃ³n modifica los valores de la tabla Q en base a la
ecuaciÃ³n descrita en la Figura 12 y se vuelva a la fase 2.
Este modelo de funcionamiento se producirÃ¡ durante un numero controlado de iteraciones debido
a que es muy complicado identificar cuando se han conseguido obtener los valores Ã³ptimos de
la tabla. Es posible identificar cuando se han obtenido los valores Ã³ptimos de la tabla Q
analizando los cambios que se producen en cada una de las iteraciones, si durante un nÃºmero
determinado de iteraciones no se produce ningÃºn cambio en la tabla, se podrÃ­a intuir que el
proceso de aprendizaje ha terminado. Pero en casos en los cuales la tabla Q a generar es de un
tamaÃ±o muy elevado es posible que sea muy complicado identificar este tipo de situaciones, por
lo que lo este tipo de estrategias no son muy utilizadas. AdemÃ¡s el algoritmo Q-Learning tiene
dos parÃ¡metros cuyos valores pueden cambiar el proceso de aprendizaje por lo que es
interesante aprender con diferente valores de estos parÃ¡metros.
4.5 Deep Reinforcement Learning
La apariciÃ³n de las tÃ©cnicas de aprendizaje profundo supuso una revoluciÃ³n debido a que
permitÃ­an crear arquitectura muchos mÃ¡s complejas que permitÃ­an resolver de complejidad muy
elevada relacionadas con los procesos de visiÃ³n y habla. Pero esta revoluciÃ³n no se detuvo en
la utilizaciÃ³n de redes de neuronas profundas para la construcciÃ³n de modelos basados en
aprendizaje supervisado o no supervisado, sino que ha terminado por alcanzar al aprendizaje
por refuerzo. Uno de los principalmente problemas de los algoritmos basados en aprendizaje por
refuerzo es la complejidad del modelo el cual puede estar formando por millones de estados y
acciones lo que supone en el caso del algoritmo Q-Learning la necesidad de una tabla Q que
tendrÃ¡ un tamaÃ±o excesivo. La necesidad de mantener en memoria una estructura de datos de
una tamaÃ±o tan elevado supone un gran problema a la hora de construir modelos en entorno
29
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
muy complejos. La posible soluciÃ³n a este problema consiste en sustituir la tabla por algÃºn otro
tipo de estructura que tenga un tamaÃ±o menor o utilizar algÃºn tipo de algoritmo que no tenga la
necesidad de un estructura de datos con grande. Una de las posibles alternativas es la utilizaciÃ³n
de las redes de neuronas, las cuales pueden ser utilizadas para construir un modelo que permita
predecir el refuerzo de todas las posibles acciones que pueden ser ejecutadas en un determinado
estado que se incluye como entrada a la red.
En base a esta idea es posible construir una red de neuronas profunda que actuÃ© como un
predictor del refuerzo de la acciones, es decir, la red utilizarÃ­a como entrada un representaciÃ³n
del estado del entorno y generarÃ­a como salida un vector donde cada uno de los elementos se
corresponderÃ­a con el valor de refuerzo de cada una de las acciones que pueden aplicarse en el
estado. La Figura 13 presentan dos posibles arquitecturas utilizadas para la construcciÃ³n de este
tipo de redes de neuronas. La estructura que se encuentra a la izquierda funcionarÃ­a de manera
similar a un regresor, dada un entrada compuesta por el estado del mundo y una posible acciÃ³n
que ser ejecutada se obtiene un valor numÃ©rico que se corresponde con la recompensa que se
obtendrÃ­a al ejecutar esta acciÃ³n. Este tipo de configuraciÃ³n es muy Ãºtil para problemas con un
nÃºmero muy elevado de acciones donde sÃ³lo un pequeÃ±o conjunto de ellas pueden ser aplicadas
en el estado. El modelo construido mediante una red con esta arquitectura permitirÃ­a obtener el
valor de refuerzo que se obtendrÃ­a por la ejecuciÃ³n de una acciÃ³n especÃ­fico, por lo que serÃ­a
necesario un sistema que utilizara la red para obtener el valor de cada una de ellas y a
continuaciÃ³n elegir la acciÃ³n que obtuvo el mejor valor. En cambio, la arquitectura presentada en
a la derecha de la Figura 13 funcionarÃ­a de manera similar a un clasificador ya que generarÃ­a
como salida un array con todos las posibles acciones que pueden ser ejecutadas en el juego
generando un valor de refuerzo para cada una de ellas de forma que se elegirÃ­a aquella acciÃ³n
el valor de refuerzo mÃ¡s grande. Este tipo de redes son muy Ãºtiles para resolver problemas en
los cuales no existe un nÃºmero muy elevado de posibles acciones o donde no es posible
identificar cuales las acciones que pueden ejecutarse en un determinado estado. El modelo
construido mediante una red con esta arquitectura permitirÃ­a obtener el valor de refuerzo de todas
las acciones que pueden ejecutarse en el entorno, por lo que serÃ­a necesario un sistema que
seleccionara la acciÃ³n con mayor valor.
Â© Structuralia
30Inteligencia artificial â€“ Apredizaje automÃ¡tico II
Figura 13: Ejemplos de las posibles estructuras de la redes de neuronas basadas en Aprendizaje AutomÃ¡tico
Este tipo de redes de neuronas han sido denominadas como Redes de Neuronas Profundas de
Aprendizaje por Refuerzo (Reinfocement Learning Deep NN) ya que permiten aprender un
modelo que funciona de manera similar a como lo harÃ­a un algoritmo de aprendizaje por refuerzo
pero utilizando una red de neuronas que actÃºa como un aproximador del refuerzo. La mayorÃ­a
de las aproximaciones que han utilizado este tipo de tÃ©cnica intentaban construir un agente que
fuera capaz de jugar en un videojuego lo que le facilitaba la representaciÃ³n de la informaciÃ³n ya
que consistÃ­a en captar una imagen de la pantalla del juego y utilizar una variaciÃ³n de una red
de neuronas profunda de tipo convolucional que realiza un preprocesado de la imagen con el fin
de extraer caracterÃ­sticas que permitan definir cual ese el posible refuerzo de cada uno de las
posibles acciones de forma que se pueda elegir aquella con mayor refuerzo. La primera
implementaciÃ³n desarrollada por los ingenieros de la empresa Deep Mind Technologies [11]
donde demostraron como una mÃ¡quina era capaz de aprender a jugar a los videojuegos de una
Atari 2600 mediante el anÃ¡lisis de los pixeles de la imagen que representaban el estado del
juego. Uno de los elementos mÃ¡s destacables de esta implementaciÃ³n es que habÃ­a sido
entrenada para jugar a siete diferentes videojuegos que utilizaban el mismo interfaz de control.
La Figura 14 presenta una representaciÃ³n parcial de la de una red similar a la utilizada para jugar
con la videoconsola Atari 2600 la cual dispone una serie de capas convolucionales que extraen
caracterÃ­sticas de la imagen y una serie de capas fully connected que clasifican la informaciÃ³n
con el fin de generar la salida.
31
Inteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
Figura 14: Ejemplo de la estructura de una Red de Neuronas de Aprendizaje Profundo. Â© DeepMind
5. CONCLUSIONES
En este tema se han presentan las tres familias restantes de mÃ©todos de aprendizaje automÃ¡tico.
Uno de ellos utilizando conjuntos de datos muy complejos a partir de los cuales es muy
complicado identificar caracterÃ­sticas que permitan construir modelos (Aprendizaje Profundo); y
dos que ofrecen diferentes algoritmos que son capaces de aprender modelos utilizando
conjuntos de datos de entrenamiento no etiquetados (Aprendizaje no Supervisado) o mediante
la interacciÃ³n en tiempo real con el entorno (Aprendizaje por Refuerzo). Cada una de estas
tÃ©cnicas estÃ¡ especializada en aprender modelos en base a diferentes representaciones en la
informaciÃ³n desde la mÃ¡s bÃ¡sica definida como un vector de atributos o caracterÃ­sticas hasta los
pixeles de una imagen. En general, podemos indicar que todas estas familias ofrecen una amplia
gama de mÃ©todos para la construcciÃ³n de modelos de aprendizaje los cuales en combinaciÃ³n
con las nuevas tÃ©cnicas de extracciÃ³n y manipulaciÃ³n de informaciÃ³n de manera masiva van a
permitir la resoluciÃ³n de muchÃ­simos problemas a los cuales no podÃ­amos enfrentarnos con la
tecnologÃ­a de hace unos aÃ±os. En el prÃ³ximo tema se describirÃ¡ como funciona el ciclo de vida

32Inteligencia artificial â€“ Apredizaje automÃ¡tico II
de los datos y como las diferentes tÃ©cnicas descritas en estos dos temas son utilizadas para
construir modelos mediante la utilizaciÃ³n masiva de datos gracias al Big Data.
6. REFERENCIAS
[1] MacQueen, J. B. (1967). Some Methods for classification and Analysis of Multivariate
Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and
Probability. University of California Press. pp. 281-297.
[2] Hamerly, G. and Elkan, C. (2002). Alternatives to the k-medias algorithm that find better
clusterings. Proceedings of the eleventh international conference on Information and knowledge
management (CIKM).
[3] de Berg M., Cheong O., Van Kreveld M., and Overmars M. (2008). Computational Geometry:
Algorithms and Applications (Tercera ediciÃ³n). Springer-Verlag BerlÃ­n Heidelberg.
[4] Kohonen, T. (1982). Self-Organized Formation of Topologically Correct Feature
Maps. Biological Cybernetics. Volumen 43(1), pp 59-69.
[5] Richard S. S. and Andrew G. B. (1998) Reinforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT Press.
[6] Bellman, R. (1957). A Markovian Decision Process. Journal of Mathematics and
Mechanics. Volumen 6.
[7] Ã…strÃ¶m, K. J. (1965). Optimal control of Markov processes with incomplete state
information. Journal of Mathematical Analysis and Applications. Volumen 10. Pp 174-205.
[8] Richard B. (1958). Dynamic programming and stochastic control processes. Information and
Control. Volumen 1(3). pp 228-239.
[9] Howard R.A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge,
Massachusetts.
[10] Bellman R. (1957). Dynamic Programming. Princeton University Press.
[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
(2013). Playing Atari with Deep Reinforcement Learning. NIPS Deep Learning Workshop.
[12] Goodfellow I., Bengio Y. and Courville, Aaron. (2016). Deep Learning. MIT Press.
[13] Mitchell T. M. (1997). Machine Learning. McGraw-Hill, New York.
33
Â© StructuraliaInteligencia artificial â€“ ResoluciÃ³n automÃ¡tica de problemas
[14] Dayan P. (1992) Technical note q-learning. Machine Learning, Volumen 292(3). Pp 279â€“292.

